{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557bab66-37dd-491e-88d7-463b84890c9f",
   "metadata": {},
   "source": [
    "# Rag-Python Glossary Q&A Bot\n",
    "A lightweight Retrieval-Augmented Generation (RAG) assistant that answers questions using the official Python 3.12 Glossary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fca694-1e8b-46b3-9f6c-f78d04a5639e",
   "metadata": {},
   "source": [
    "### 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03305e77-88ad-494f-a301-38596249e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Rag app\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880eddc-aa21-4426-9078-54dcccba3be9",
   "metadata": {},
   "source": [
    "### 2. CONFIGURATION â€“ OPENROUTER API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b978a629-a674-49a4-938c-3b1bebd2c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key used successfully!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv('OPENROUTER_API_KEY')\n",
    "API_KEY = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "print(\"API key used successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165aeef-203e-4c00-b6a2-f1eeae0163a0",
   "metadata": {},
   "source": [
    "### 3. LOAD AND EXTRACT TEXT FROM PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53600d10-024f-4128-9fe0-96712a897e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF ->\n",
      "   Extracted text from 18 pages\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading PDF ->\")\n",
    "reader = PdfReader(\"python_glossary.pdf\")\n",
    "text = \"\\n\".join(page.extract_text() + \"\\n\" for page in reader.pages)\n",
    "print(f\"   Extracted text from {len(reader.pages)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfe04f-0f0c-491f-b357-be60e4f572d8",
   "metadata": {},
   "source": [
    "### 4. CHUNK TEXT INTO SENTENCES USING spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b63778-1f5b-436e-8d5b-4227c2afba4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text into semantic chunks with spaCy...\n",
      "   Created 172 semantic chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking text into semantic chunks with spaCy...\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "chunks = []\n",
    "current_chunk = \"\"\n",
    "MAX_CHUNK_LEN = 350  # characters\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sentence = sent.text.strip()\n",
    "\n",
    "    # Skip very small or useless lines\n",
    "    if len(sentence) < 5:\n",
    "        continue\n",
    "\n",
    "    # If chunk becomes too large, save it and start new\n",
    "    if len(current_chunk) + len(sentence) > MAX_CHUNK_LEN:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        current_chunk = sentence\n",
    "    else:\n",
    "        current_chunk += \" \" + sentence\n",
    "\n",
    "# Add last chunk\n",
    "if current_chunk:\n",
    "    chunks.append(current_chunk.strip())\n",
    "\n",
    "print(f\"   Created {len(chunks)} semantic chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdb7a7-2f4d-42ca-bbe9-04c4340497e9",
   "metadata": {},
   "source": [
    "### 5. LOAD HUGGING FACE EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c143f3-f8d6-41c3-8c96-add8c16cc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model ->\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model ->\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd8071-f0d5-48d9-8c8b-1c64411e141f",
   "metadata": {},
   "source": [
    "### 6. SETUP CHROMA VECTOR DATABASE (FAISS backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab688d96-3f93-4a89-b644-36cfb2ee8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB ->\n",
      "   Loading existing database from 'glossary_db/'\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing ChromaDB ->\")\n",
    "client = chromadb.PersistentClient(path=\"glossary_db\")\n",
    "collection = client.get_or_create_collection(name=\"glossary\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(\"   First run detected, embedding and saving chunks...\")\n",
    "    embeddings = embedder.encode(chunks, show_progress_bar=True).tolist()\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    )\n",
    "    print(\"   Database built and saved to 'glossary_db/'\")\n",
    "else:\n",
    "    print(\"   Loading existing database from 'glossary_db/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f68da-1e88-44e2-b45a-c005c37d8719",
   "metadata": {},
   "source": [
    "### 7. INITIALIZE LLM VIA OPENROUTER (Llama 3.1 8B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b777588-26ca-4197-8237-c82661a90254",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9d5ff-8754-45ff-b26d-edeab828708f",
   "metadata": {},
   "source": [
    "### 8. INTERACTIVE Q&A LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1abffe4f-3059-4862-a385-9931c97d64a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "     Rag-Python Glossary Q&A Bot is READY!     \n",
      "     Powered by Llama 3.1 8B Instruct       \n",
      "     Type 'quit', 'exit', or 'bye' to stop  \n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How to use print command?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating answer...     \n",
      "RAG-Bot: The `print` command is used to output text or values to the screen. It can be used in various ways, such as:\n",
      "\n",
      "* `print(\"Hello, World!\")` to print a string\n",
      "* `print(123)` to print an integer\n",
      "* `print(food)` to print the contents of a list or other iterable\n",
      "\n",
      "In the context of the provided examples, `print` is used to output the elements of the `food` list, like this: `print(piece)`.\n",
      "\n",
      "Note that the `print` command is not explicitly defined in the provided context, but it is a built-in function in Python that is commonly used for outputting values.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Happy learning! Come back anytime.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"     Rag-Python Glossary Q&A Bot is READY!     \")\n",
    "print(\"     Powered by Llama 3.1 8B Instruct       \")\n",
    "print(\"     Type 'quit', 'exit', or 'bye' to stop  \")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"You:\").strip()\n",
    "\n",
    "    if question.lower() in {\"quit\", \"exit\", \"bye\", \"q\"}:\n",
    "        print(\"\\nHappy learning! Come back anytime.\")\n",
    "        break\n",
    "\n",
    "    if not question:\n",
    "        continue\n",
    "\n",
    "    print(\"   Searching glossary...\", end=\"\\r\")\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    query_embedding = embedder.encode([question]).tolist()\n",
    "    results = collection.query(query_embeddings=query_embedding, n_results=5)\n",
    "    context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "\n",
    "    print(\"   Generating answer...     \")\n",
    "\n",
    "    # Call LLM with context\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a precise and helpful Python documentation assistant. \"\n",
    "                           \"Answer questions using ONLY the provided glossary context. \"\n",
    "                           \"Be concise, accurate, and professional.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    print(f\"RAG-Bot: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG Env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
